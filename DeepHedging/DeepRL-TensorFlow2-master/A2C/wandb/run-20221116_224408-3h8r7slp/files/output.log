1/1 [==============================] - 0s 81ms/step
1/1 [==============================] - 0s 23ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 9ms/step
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 7ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 9ms/step
1/1 [==============================] - 0s 9ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 9ms/step
1/1 [==============================] - 0s 8ms/step
1/1 [==============================] - 0s 7ms/step
Traceback (most recent call last):
  File "/Users/daehungo/Desktop/DataCapstone/github/DeepHedging/DeepRL-TensorFlow2-master/A2C/A2C_Continuous.py", line 184, in <module>
    main()
  File "/Users/daehungo/Desktop/DataCapstone/github/DeepHedging/DeepRL-TensorFlow2-master/A2C/A2C_Continuous.py", line 180, in main
    agent.train()
  File "/Users/daehungo/Desktop/DataCapstone/github/DeepHedging/DeepRL-TensorFlow2-master/A2C/A2C_Continuous.py", line 161, in train
    actor_loss = self.actor.train(states, actions, advantages)
  File "/Users/daehungo/Desktop/DataCapstone/github/DeepHedging/DeepRL-TensorFlow2-master/A2C/A2C_Continuous.py", line 63, in train
    self.opt.apply_gradients(zip(grads, self.model.trainable_variables))
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py", line 738, in apply_gradients
    return tf.__internal__.distribute.interim.maybe_merge_call(
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py", line 51, in maybe_merge_call
    return fn(strategy, *args, **kwargs)
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py", line 797, in _distributed_apply
    update_op = distribution.extended.update(
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2633, in update
    return self._update(var, fn, args, kwargs, group)
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py", line 3706, in _update
    return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py", line 3712, in _update_non_slot
    result = fn(*args, **kwargs)
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py", line 595, in wrapper
    return func(*args, **kwargs)
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py", line 776, in apply_grad_to_update_var
    update_op = self._resource_apply_dense(grad, var, **apply_kwargs)
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py", line 177, in _resource_apply_dense
    return tf.raw_ops.ResourceApplyAdam(
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/tensorflow/python/util/tf_export.py", line 412, in wrapper
    return f(**kwargs)
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/tensorflow/python/ops/gen_training_ops.py", line 1427, in resource_apply_adam
    _ops.raise_from_not_ok_status(e, name)
  File "/Users/daehungo/yes/envs/colab/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 7209, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation ResourceApplyAdam: Could not satisfy explicit device specification '/job:localhost/replica:0/task:0/device:GPU:0' because no supported kernel for GPU devices is available.
Colocation Debug Info:
Colocation group had the following types and supported devices:
Root Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
ResourceApplyAdam: CPU
_Arg: GPU CPU
Colocation members, user-requested devices, and framework assigned devices, if any:
  var (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  m (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  v (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0
  ResourceApplyAdam (ResourceApplyAdam) /job:localhost/replica:0/task:0/device:GPU:0
Op: ResourceApplyAdam
Node attrs: use_locking=true, T=DT_DOUBLE, use_nesterov=false
Registered kernels:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_COMPLEX128]
	 [[{{node ResourceApplyAdam}}]] [Op:ResourceApplyAdam]