#print(isinstance(tf_env, tf_environment.TFEnvironment)) # tf_env가 2번째 arg 타입 맞는지 boolean으로 return
#print("TimeStep Specs:", tf_env.time_step_spec())
'''
TimeStep Specs: TimeStep(
{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., 
dtype=float32), maximum=array(1., dtype=float32)),
 'observation': BoundedTensorSpec(shape=(1,), dtype=tf.int32, name='observation', minimum=array(0), maximum=array(2147483647)),
 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),
 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})
Action Specs: BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0), maximum=array(2))
'''

'''
train_py_env = wrappers.TimeLimit(env, duration=100) #change duration later if neded
eval_py_env = wrappers.TimeLimit(env, duration=100)

train_env = tf_py_environment.TFPyEnvironment(train_py_env)
eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)
'''



'''

environment = HedgeENV(S,T)
tf_env = tf_py_environment.TFPyEnvironment(environment)
time_step = tf_env.reset()
rewards = []
steps = []
num_episodes = 5

#print(S.shape) # (31,1)

#print(time_step)
'''
'''
TimeStep(
{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,        
 'observation': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]])>,
 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,
 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>})
'''
#print(time_step.is_last()) #tf.Tensor([False], shape=(1,), dtype=bool)
'''
for _ in range(num_episodes):
  episode_reward = 0
  episode_steps = 0
  while not time_step.is_last():
    action = np.array(tf.random.uniform([1], 0, 3, dtype=tf.int32))
    print(action)
    time_step = tf_env.step(action)
    episode_steps += 1
    episode_reward += time_step.reward.numpy()

  rewards.append(episode_reward)
  steps.append(episode_steps)
  time_step = tf_env.reset()

num_steps = np.sum(steps)
avg_length = np.mean(steps)
avg_reward = np.mean(rewards)

print('num_episodes:', num_episodes, 'num_steps:', num_steps)
print('avg_length', avg_length, 'avg_reward:', avg_reward)
'''